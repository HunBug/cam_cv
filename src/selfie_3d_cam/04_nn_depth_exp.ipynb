{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline cannot infer suitable model classes from vinvino02/glpn-nyu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\akoss\\work\\hunbug\\repos\\cam_cv\\src\\selfie_3d_cam\\04_nn_depth_exp.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/akoss/work/hunbug/repos/cam_cv/src/selfie_3d_cam/04_nn_depth_exp.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvinvino02/glpn-nyu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/akoss/work/hunbug/repos/cam_cv/src/selfie_3d_cam/04_nn_depth_exp.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m depth_estimator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mdepth-estimation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mcheckpoint)\n",
      "File \u001b[1;32mc:\\Users\\akoss\\work\\hunbug\\repos\\cam_cv\\.conda\\lib\\site-packages\\transformers\\pipelines\\__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    787\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 788\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    789\u001b[0m         model,\n\u001b[0;32m    790\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    791\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    792\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    793\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    794\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    795\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    796\u001b[0m     )\n\u001b[0;32m    798\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    799\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\akoss\\work\\hunbug\\repos\\cam_cv\\.conda\\lib\\site-packages\\transformers\\pipelines\\base.py:251\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     class_tuple \u001b[39m=\u001b[39m class_tuple \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(classes)\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(class_tuple) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline cannot infer suitable model classes from \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m model_class \u001b[39min\u001b[39;00m class_tuple:\n\u001b[0;32m    254\u001b[0m     kwargs \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;31mValueError\u001b[0m: Pipeline cannot infer suitable model classes from vinvino02/glpn-nyu"
     ]
    }
   ],
   "source": [
    "checkpoint = \"vinvino02/glpn-nyu\"\n",
    "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../../data/private\")\n",
    "# VIDEO_NAME = Path(\"VID_20230814_102451798.mp4\")\n",
    "VIDEO_NAME = Path(\"moa.mp4\")\n",
    "\n",
    "# check if the video exists\n",
    "if not (DATA_ROOT / VIDEO_NAME).exists():\n",
    "    raise FileNotFoundError(f\"Video not found: {DATA_ROOT / VIDEO_NAME}\")\n",
    "\n",
    "# open video file with opencv\n",
    "video_reader = cv2.VideoCapture(str(DATA_ROOT/VIDEO_NAME))\n",
    "\n",
    "# check if video opened successfully\n",
    "if (video_reader.isOpened() == False):\n",
    "    raise Exception(f\"Error opening video stream or file: {DATA_ROOT/VIDEO_NAME}\")\n",
    "    \n",
    "width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# print video properties\n",
    "print(\"Video properties:\")\n",
    "print(\"  - frame width:  \", width)\n",
    "print(\"  - frame height: \", height)\n",
    "print(\"  - frame count:  \", frames_count)\n",
    "print(\"  - frame rate:   \", video_reader.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_reader.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "_, img1 = video_reader.read()\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = depth_estimator(img1)\n",
    "depth = predictions['depth']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
